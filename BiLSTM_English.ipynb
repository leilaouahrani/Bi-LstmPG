{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "##Copyright    O., Hamel et S. Lamari for The C00L07UN100120180002 Project.##########################\n",
    "#####################################################################################################\n",
    "\n",
    "#this file contains the code relating to the training of the Bi LSTM paraphrase generation model (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rBcYMHUnQKcP",
    "outputId": "a2a53300-fdfa-4474-82db-e4eac64969f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "# import the linguistic resources\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import numpy as np\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1pZjMamlLia5rRd0KEmLu-NnsIFBF3NKB'}) \n",
    "downloaded.GetContentFile('W2V.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1OO6h6FH28KVRWMFJ25pA7dO_FH9t-THU'}) \n",
    "downloaded.GetContentFile('W2I.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1YhtngRWwPlZoQ1IuRa_sGk_JXxfu9gZT'}) \n",
    "downloaded.GetContentFile('I2W.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1eMsbuwJMnVBt9YPexoy58-pANJrhHddp'}) \n",
    "downloaded.GetContentFile('I2V.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'195SH3to94kQPZMkZadD4v04px_jkOmBN'}) \n",
    "downloaded.GetContentFile('questions.csv')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1-13mgnkRzDpIfT_f_-EK-WlyJsRBeckj'}) \n",
    "downloaded.GetContentFile('model.bin')\n",
    "\n",
    "W2I,I2W,W2V,I2V = {},{},{},{}\n",
    "\n",
    "data = np.load('W2V.npy', allow_pickle =True)\n",
    "W2V.update(data.item()) \n",
    "\n",
    "data = np.load('W2I.npy', allow_pickle =True) \n",
    "W2I.update(data.item()) \n",
    "\n",
    "data =  np.load('I2W.npy', allow_pickle =True)\n",
    "I2W.update(data.item())\n",
    "\n",
    "data =  np.load('I2V.npy', allow_pickle =True)\n",
    "I2V.update(data.item())\n",
    "\n",
    "c = np.zeros(100)\n",
    "I2V[0]=c\n",
    "c=[]\n",
    "\n",
    "downloaded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "P38Y0kzM-BWh",
    "outputId": "aa5879a9-3b8d-4242-bd46-b3c6196e3390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "kv22kSC5X48C",
    "outputId": "5e41ab87-30da-4df5-a547-4e0817db872b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4027169 word vectors of word2vec\n",
      "Processing text dataset\n",
      "Found 404351 texts in questions.csv\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Found 383898 texts in questions.csv\n",
      "Found 72397 unique tokens\n",
      "Shape of data tensor: (383898, 30)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "\n",
    "# define parameters\n",
    "\n",
    "TRAIN_DATA_FILE = 'questions.csv'\n",
    "EMBEDDING_FILE = 'model.bin'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# index word vectors\n",
    "\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n",
    "\n",
    "\n",
    "# process texts in datasets\n",
    "\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" \n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "print('Found %s texts in questions.csv' % len(texts_1))\n",
    "\n",
    "# return only sentences that have a size less than or equal to 30, all of whose words are in the WE vocabulary\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "t1, t2 = [], []\n",
    "for i in range(len(texts_1)):\n",
    "\n",
    "  wordList = nltk.word_tokenize(texts_1[i])\n",
    "  test = True\n",
    "  for e in wordList:\n",
    "    test = test and e in word2vec.vocab\n",
    "  if test == True and len(wordList) <= 30:\n",
    "    t1.append(texts_1[i])\n",
    "    t2.append(texts_2[i])\n",
    "\n",
    "texts_1 = t1 \n",
    "texts_2 = t2\n",
    "\n",
    "t1, t2 = [], []\n",
    "for i in range(len(texts_2)):\n",
    "  wordList = nltk.word_tokenize(texts_2[i])\n",
    "  test = True\n",
    "  for e in wordList:\n",
    "    test = test and e in word2vec.vocab\n",
    "  if test == True and len(wordList) <= 30:\n",
    "    t1.append(texts_1[i])\n",
    "    t2.append(texts_2[i])\n",
    "\n",
    "texts_1 = t1 \n",
    "texts_2 = t2 \n",
    "\n",
    "print('Found %s texts in questions.csv' % len(texts_1))\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "print('Shape of data tensor:', data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WAECA4AWqwCW",
    "outputId": "ca914bda-fb1f-49de-e107-b66ba3bb3275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:\t383898\n"
     ]
    }
   ],
   "source": [
    "# import data science packages\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    " \n",
    "from keras.utils import to_categorical \n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "\n",
    "num_train_patters = len(data_1)\n",
    "\n",
    "print('Total patterns:\\t' + str(num_train_patters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "7_gqD2Owq9uR",
    "outputId": "618afa00-36fc-4c5a-b1f5-b3b14abe4d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "LSTM model built.\n"
     ]
    }
   ],
   "source": [
    "#define parameters\n",
    "\n",
    "rnn_size = 256 # size of RNN\n",
    "learning_rate = 0.0001 #learning rate\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "#create the model\n",
    "\n",
    "print('Building LSTM model...')\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\",return_sequences = True),input_shape=(30, 100)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(I2V),activation=\"softmax\"))\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "loss_function_used = 'categorical_crossentropy'\n",
    "callbacks=[EarlyStopping(patience=2, monitor='val_acc')]\n",
    "model.compile(optimizer='Adam',loss= loss_function_used,metrics=['acc'])\n",
    "print('LSTM model built.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpreFIr1qzcW"
   },
   "outputs": [],
   "source": [
    "# data partition part\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import csv\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1, n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        global data_2\n",
    "        global data_1\n",
    "        self.x_data,self.y_data= data_1, data_2\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "\n",
    "        global I2V\n",
    "\n",
    "        X = np.zeros((self.batch_size, 30,100),'float64')\n",
    "        Y = np.zeros((self.batch_size, 30,len(I2V)),'float64')\n",
    "        \n",
    "\n",
    "        #for i, ID in enumerate(list_IDs_temp):\n",
    "        for i,ID in enumerate(list_IDs_temp) :\n",
    "            sentence = self.x_data[ID]\n",
    "            paraphrase = self.y_data[ID]\n",
    "            pos = 0\n",
    "            for j in sentence:\n",
    "                X[i, pos,] = I2V[j]\n",
    "                pos = pos+1\n",
    "            pos = 0\n",
    "            for j in paraphrase:\n",
    "                Y[i, pos,j] = 1\n",
    "                pos = pos+1\n",
    "\n",
    "                \n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "HupPVCOGrClD",
    "outputId": "3615f062-a80a-44c6-da7c-7433da7a1828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8396/8397 [============================>.] - ETA: 1s - loss: 1.1921e-07 - acc: 0.6392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:616: UserWarning: The input 19 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#Train part\n",
    "\n",
    "import keras \n",
    "\n",
    "params = {'dim': (32,32,32),\n",
    "          'batch_size': 32,\n",
    "          'n_channels': 1,\n",
    "          'n_classes': 6,\n",
    "          'shuffle': True\n",
    "          }\n",
    "\n",
    "# Datasets\n",
    "\n",
    "trainingSet = [i for i in range(0,307118)]\n",
    "validationSet = trainingSet[268728:307118]\n",
    "trainingSet = trainingSet[0:268728]\n",
    "\n",
    "\n",
    "partition = {}\n",
    "partition['train'] = trainingSet\n",
    "partition['validation'] = validationSet\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], data_2, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], data_2, **params)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "recuperate the model in order to continue training\n",
    "\n",
    "model = keras.models.load_model('/content/drive/My Drive/English DataSet 1sens/model.h5')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "acc  = float('-inf')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#start the train\n",
    "\n",
    "for i in range (500):\n",
    "\n",
    "  history = model.fit_generator(generator=training_generator,validation_data=validation_generator,epochs = 1,use_multiprocessing=True,workers=1)\n",
    "  \n",
    "    \n",
    "  #recuperation of loss & acc\n",
    "  curr_loss = history.history['loss'][0]\n",
    "  curr_acc = history.history['acc'][0]\n",
    "  curr_val_acc = history.history['val_accuracy'][0] \n",
    "  curr_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "  loss_file=open(\"/content/drive/My Drive/English DataSet 1sens/lossOneHotBiLSTM.txt\", \"a+\")\n",
    "  acc_file=open(\"/content/drive/My Drive/English DataSet 1sens/accOneHotBiLSTM.txt\", \"a+\")\n",
    "  loss_file.write('\\n'+str(curr_loss))\n",
    "  acc_file.write('\\n'+str(curr_acc))\n",
    "  \n",
    "  loss_file.close()\n",
    "  acc_file.close()\n",
    "\n",
    "  #saving model\n",
    "\n",
    "  if curr_acc > acc :\n",
    "    model.save('/content/drive/My Drive/English DataSet 1sens/bestmodel.h5')\n",
    "    acc = curr_acc\n",
    "  else:\n",
    "    model.save('/content/drive/My Drive/English DataSet 1sens/model.h5')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BiLSTM_English_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
