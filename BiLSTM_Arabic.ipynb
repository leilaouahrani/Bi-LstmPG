{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "##Copyright    O., Hamel et S. Lamari for The C00L07UN100120180002 Project.##########################\n",
    "#####################################################################################################\n",
    "\n",
    "#this file contains the code relating to the training of the Bi LSTM paraphrase generation model (Arabic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "P38Y0kzM-BWh",
    "outputId": "70763ad6-ecc0-4cef-cb6e-768eda435a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBcYMHUnQKcP"
   },
   "outputs": [],
   "source": [
    "# import the linguistic resources\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import numpy as np\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1Dn7Dm5qQhGB2zEG38Vo3OgN2t6_5KUFs'}) \n",
    "downloaded.GetContentFile('W2V.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1We-uATZvQ5Qm2LM0f8z6yfHzS6i6FmV6'}) \n",
    "downloaded.GetContentFile('W2I.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1rlYGeW2HQK4umbRD0c-uzCuEiKUmbVGq'}) \n",
    "downloaded.GetContentFile('I2W.npy')\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1wWjjQe4QwZLDKvq_iKsQVIFYzPxqfQrH'}) \n",
    "downloaded.GetContentFile('Classeur2.csv')\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read and clean the dataset\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "file=open(\"Classeur2.csv\",\"r\",encoding=\"utf8\")\n",
    "test=csv.reader(file)\n",
    "i = -1\n",
    "for row in test:\n",
    "    sentence = ''.join(map(str, row))  #convertir la list en string\n",
    "    wordList = sentence.split()        # convertir la phrase en une liste de mots\n",
    "    \n",
    "    i = i+1\n",
    "    if i%2==0:\n",
    "        x.append(wordList)\n",
    "    else:\n",
    "        y.append(wordList)\n",
    "\n",
    "word_to_vec,word_to_int,int_to_word,data = {},{},{},{}\n",
    "\n",
    "data = np.load('W2V.npy', allow_pickle =True)\n",
    "word_to_vec.update(data.item()) \n",
    "\n",
    "data = np.load('W2I.npy', allow_pickle =True) \n",
    "word_to_int.update(data.item()) \n",
    "\n",
    "data =  np.load('I2W.npy', allow_pickle =True)\n",
    "int_to_word.update(data.item())\n",
    "\n",
    "c = np.zeros(300)\n",
    "word_to_vec[' ']=c\n",
    "c=[]\n",
    "\n",
    "\n",
    "word_to_int = {k:v+1 for k,v in word_to_int.items()}\n",
    "word_to_int[' '] = 0\n",
    "\n",
    "int_to_word = {v:k for k,v in word_to_int.items()}\n",
    "\n",
    "\n",
    "int_to_vec={}\n",
    "\n",
    "int_to_vec = {i:word_to_vec[int_to_word[i]] for i in int_to_word.keys()}\n",
    "\n",
    "print('la taille de x : ',len(x))\n",
    "print('la taille de y : ',len(y))\n",
    "\n",
    "downloaded = []\n",
    "\n",
    "data = [(e1,e2) for e1,e2 in zip(x,y) if len(e1)<=30 and len(e2)<=30] \n",
    "\n",
    "print(len(data))\n",
    "\n",
    "x_data = [] # list of lists\n",
    "y_data = [] # list of lists\n",
    "\n",
    "for (e1,e2) in data : \n",
    "  e1 = e1 + [' ']* (30 - len (e1))\n",
    "  e2 = e2 + [' ']* (30 - len (e2))\n",
    "\n",
    "  x_data.append(e1)\n",
    "  \n",
    "  y_data.append(e2)\n",
    "  \n",
    "\n",
    "print(len(x_data))\n",
    "print(len(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bvRHjz81g5Vh",
    "outputId": "687cbd8c-f2a5-4686-a2aa-12afe31a8dde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:\t81102\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "\n",
    "num_train_patters = len(x_data)\n",
    "print('Total patterns:\\t' + str(num_train_patters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "7_gqD2Owq9uR",
    "outputId": "16941ae5-28a7-42c7-deff-a5cdf82a07d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model...\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "LSTM model built.\n"
     ]
    }
   ],
   "source": [
    "#define parameters\n",
    "\n",
    "rnn_size = 256 # size of RNN\n",
    "learning_rate = 0.0001 #learning rate\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Building LSTM model...')\n",
    "\n",
    "#create the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\",return_sequences = True),input_shape=(30, 300)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(int_to_vec),activation=\"softmax\"))\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "loss_function_used = 'categorical_crossentropy'\n",
    "callbacks=[EarlyStopping(patience=2, monitor='val_acc')]\n",
    "model.compile(optimizer='Adam',loss= loss_function_used,metrics=['acc'])\n",
    "print('LSTM model built.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpreFIr1qzcW"
   },
   "outputs": [],
   "source": [
    "# data partition part\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import csv\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1, n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        global x_data\n",
    "        global y_data\n",
    "        self.x_data,self.y_data= x_data, y_data\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "\n",
    "        global word_to_vec\n",
    "        global word_to_int\n",
    "\n",
    "        X = np.zeros((self.batch_size, 30,300),'float64')\n",
    "        Y = np.zeros((self.batch_size, 30,len(word_to_vec)),'float64')\n",
    "        \n",
    "\n",
    "        #for i, ID in enumerate(list_IDs_temp):\n",
    "        for i,ID in enumerate(list_IDs_temp) :\n",
    "            sentence = self.x_data[ID]\n",
    "            paraphrase = self.y_data[ID]\n",
    "            pos = 0\n",
    "            for j in sentence:\n",
    "                X[i, pos,] = word_to_vec[j]\n",
    "                pos = pos+1\n",
    "            pos = 0\n",
    "            for j in paraphrase:\n",
    "                Y[i, pos,word_to_int[j]] = 1\n",
    "                pos = pos+1\n",
    "\n",
    "                \n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HupPVCOGrClD"
   },
   "outputs": [],
   "source": [
    "#Train part\n",
    "\n",
    "import keras \n",
    "\n",
    "params = {'dim': (32,32,32),\n",
    "          'batch_size': 32,\n",
    "          'n_channels': 1,\n",
    "          'n_classes': 6,\n",
    "          'shuffle': True\n",
    "          }\n",
    "\n",
    "# Datasets\n",
    "\n",
    "trainingSet = [i for i in range(0,64881)]\n",
    "validationSet = trainingSet[48661:64881]\n",
    "trainingSet = trainingSet[0:48661]\n",
    "\n",
    "\n",
    "partition = {}\n",
    "partition['train'] = trainingSet\n",
    "partition['validation'] = validationSet\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], y_data, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], y_data, **params)\n",
    "\n",
    "\"\"\"\n",
    "recuperate the model in order to continue training\n",
    "model = keras.models.load_model('/content/drive/My Drive/Arabic DataSet 1sens 32/bestmodel.h5')\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "acc  = float('-inf')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "for i in range (500):\n",
    "\n",
    "  history = model.fit_generator(generator=training_generator,validation_data=validation_generator,epochs = 1,use_multiprocessing=True,workers=1)\n",
    "  \n",
    "  #recuperation of loss & acc\n",
    "\n",
    "  curr_loss = history.history['loss'][0]\n",
    "  curr_acc = history.history['acc'][0]\n",
    "  curr_val_acc = history.history['val_acc'][0] \n",
    "  curr_val_loss = history.history['val_loss'][0]\n",
    "\n",
    "  loss_file=open(\"/content/drive/My Drive/Arabic DataSet 1sens 32/lossOneHotBiLSTM.txt\", \"a+\")\n",
    "  acc_file=open(\"/content/drive/My Drive/Arabic DataSet 1sens 32/accOneHotBiLSTM.txt\", \"a+\")\n",
    "                                                                            \n",
    "  Valloss_file=open(\"/content/drive/My Drive/Arabic DataSet 1sens 32/VallossOneHotBiLSTM.txt\", \"a+\")\n",
    "  Valacc_file=open(\"/content/drive/My Drive/Arabic DataSet 1sens 32/ValaccOneHotBiLSTM.txt\", \"a+\")\n",
    "                                                                 \n",
    "  Valloss_file.write('\\n'+str(curr_val_loss))\n",
    "  Valacc_file.write('\\n'+str(curr_val_acc))\n",
    "\n",
    "  loss_file.write('\\n'+str(curr_loss))\n",
    "  acc_file.write('\\n'+str(curr_acc))\n",
    "  \n",
    "  loss_file.close()\n",
    "  acc_file.close()\n",
    "  Valacc_file.close()\n",
    "  Valloss_file.close()\n",
    "  \n",
    "  #saving model\n",
    "\n",
    "  if curr_acc > acc :\n",
    "    model.save('/content/drive/My Drive/Arabic DataSet 1sens 32/bestmodel.h5')\n",
    "    acc = curr_acc\n",
    "  else:\n",
    "    model.save('/content/drive/My Drive/Arabic DataSet 1sens 32/model.h5')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BiLSTM_Arabic_modif_32.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
